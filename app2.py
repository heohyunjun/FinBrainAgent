import os
import logging
import json
from datetime import datetime
from uuid import uuid4
from typing import Annotated, Optional, List
from contextlib import asynccontextmanager

from dotenv import load_dotenv
from pydantic import BaseModel
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from langchain_core.messages import HumanMessage, AnyMessage
from langchain_core.runnables.config import RunnableConfig
from langgraph.graph.message import add_messages

from langchain_mcp_adapters.client import MultiServerMCPClient
from utils.mcp_tool_mapping import bind_agent_tools
from agents.agent_library import agent_configs
from main_graph import build_graph
import time
import asyncio
import platform

# if platform.system() == "Windows":
#     asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

load_dotenv()

# =======================
# ë¡œê·¸ ì„¤ì •
# =======================
log_dir = os.path.join(os.path.dirname(__file__), "logs")
os.makedirs(log_dir, exist_ok=True)

today = datetime.now().strftime("%Y-%m-%d")
log_path = os.path.join(log_dir, f"{today}.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_path, encoding="utf-8")
    ]
)

logger = logging.getLogger(__name__)

# =======================
# FastAPI Lifespan ì„¤ì •
# =======================
@asynccontextmanager
async def lifespan(app: FastAPI):
    if platform.system() == "Windows":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    config_path = "mcp_config.json"
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            raw_config = json.load(f)
        mcp_config = raw_config.get("mcpServers", {})
        
    else:
        mcp_config = {}

    client = MultiServerMCPClient(mcp_config)
    await client.__aenter__()
    print(2222222222)
    app.state.mcp_client = client

    app.state.mcp_tools = client.get_tools()
    
    logger.info(f"MCP ë„êµ¬ {len(app.state.mcp_tools)}ê°œ ë¡œë“œë¨")


    resolved_configs = bind_agent_tools(agent_configs, app.state.mcp_tools)
    app.state.main_graph = build_graph(resolved_configs)
    logger.info("LangGraph ì´ˆê¸°í™” ì™„ë£Œ")

    yield  # ì•± ì‹¤í–‰ ì‹œì‘

    # ì¢…ë£Œ ì²˜ë¦¬
    client = getattr(app.state, "mcp_client", None)
    if client:
        await client.__aexit__(None, None, None)
        logger.info("ğŸ§¹ MCP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì™„ë£Œ")

# =======================
# FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# =======================
app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("FRONTEND_ORIGIN")],
    allow_methods=["*"],
    allow_headers=["*"],
)

# =======================
# ë°ì´í„° êµ¬ì¡°
# =======================
class UserInput(BaseModel):
    message: str
    thread_id: Optional[str] = None

class AgentState(BaseModel):
    query: str
    messages: Annotated[List[AnyMessage], add_messages]

# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
memory_store = {}  # key: thread_id, value: List[BaseMessage]

# =======================
# API ì—”ë“œí¬ì¸íŠ¸
# =======================
@app.post("/chat")
async def handle_chat(request: UserInput):
    user_message = request.message
    thread_id = request.thread_id or str(uuid4())
    config = RunnableConfig(configurable={"thread_id": thread_id})

    logger.info(f"ìˆ˜ì‹ ëœ ë©”ì‹œì§€: {user_message}")
    logger.info(f"thread_id: {thread_id}")

    prev_messages = memory_store.get(thread_id, [])
    human_msg = HumanMessage(content=user_message)

    state = AgentState(
        query=user_message,
        messages=prev_messages + [human_msg]
    )

    try:
        graph = app.state.main_graph
        response = await graph.ainvoke(state, config=config)

        if not response.get("messages"):
            raise ValueError("LangGraph ì‘ë‹µì— ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        ai_msg = response["messages"][-1]
        assistant_content = ai_msg.content

        memory_store[thread_id] = prev_messages + [human_msg, HumanMessage(content=assistant_content)]

        logger.info(f"LangGraph ì‘ë‹µ ìƒì„± ì™„ë£Œ: {assistant_content}")

        return {
            "thread_id": thread_id,
            "response": assistant_content,
            "status": "ok",
            "error_message": None
        }

    except Exception as e:
        memory_store[thread_id] = prev_messages + [human_msg]
        assistant_content = "AI ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        logger.error(f"LangGraph ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

        return {
            "thread_id": thread_id,
            "response": assistant_content,
            "status": "error",
            "error_message": str(e)
        }